# # === ä¸­å¤®å€åŸŸé¡è‰²åˆ†æï¼ˆæ¯”ä¾‹åˆ‡ + å…§ç¸®ï¼‰ ===
# CENTER_RATIO = 0.6  # å–çŸ­é‚Šçš„ 50% ç•¶ä¸­å¿ƒæ–¹å¡Šï¼›å¯è©¦ 0.40~0.60
# MARGIN_RATIO = 0.06  # å…ˆæŠŠæ•´å€‹è£åˆ‡åœ–å››é‚Šå…§ç¸® 6%ï¼Œé¿å…é‚Šç·£èƒŒæ™¯
#
# h, w = cropped.shape[:2]
# # å…ˆåšã€Œå…§ç¸®æ¡†ã€ä»¥é¿é–‹é‚Šç·£é›œè¨Š
# mx = int(w * MARGIN_RATIO)
# my = int(h * MARGIN_RATIO)
# ix1, iy1 = mx, my
# ix2, iy2 = max(w - mx, ix1 + 1), max(h - my, iy1 + 1)
# inner = cropped[iy1:iy2, ix1:ix2].copy()
#
# # åœ¨ã€Œå…§ç¸®æ¡†ã€å…§ä»¥æ¯”ä¾‹åˆ‡ä¸­å¿ƒæ–¹å¡Š
# ih, iw = inner.shape[:2]
# side = max(1, int(min(iw, ih) * CENTER_RATIO))
# cx, cy = iw // 2, ih // 2
# x1 = max(cx - side // 2, 0)
# y1 = max(cy - side // 2, 0)
# x2 = min(cx + side // 2, iw)
# y2 = min(cy + side // 2, ih)
# cropped2 = inner[y1:y2, x1:x2].copy()
#
# # --- debug åœ– ---
# def _img_to_b64(img_rgb):
#     img_bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)
#     ok, buf = cv2.imencode(".jpg", img_bgr, [int(cv2.IMWRITE_JPEG_QUALITY), 90])
#     return f"data:image/jpeg;base64,{base64.b64encode(buf).decode('utf-8')}" if ok else None
#
# # ç–Šæ¡†ï¼šå…ˆç•«å…§ç¸®æ¡†ï¼Œå†ç•«ä¸­å¿ƒæ–¹å¡Šï¼ˆå…©å±¤åº§æ¨™ï¼‰
# overlay = cropped.copy()
# # å…§ç¸®æ¡†ï¼ˆè—è‰²ï¼‰
# cv2.rectangle(overlay, (ix1, iy1), (ix2, iy2), (255, 0, 0), 2)
# # ä¸­å¿ƒæ–¹å¡Šï¼ˆç¶ è‰²ï¼‰ï¼Œè¦åŠ å›å…§ç¸®åç§»é‡
# cv2.rectangle(overlay, (ix1 + x1, iy1 + y1), (ix1 + x2, iy1 + y2), (0, 255, 0), 2)
#
# center_b64 = _img_to_b64(cropped2)
# overlay_b64 = _img_to_b64(overlay)
# cropped_b64 = _img_to_b64(cropped)  # å®Œæ•´è£åˆ‡
#
# # --- å–å¾—ä¸»è‰²ï¼ˆ4 å€¼ï¼šRGB/HEX/HSV/ä½”æ¯”ï¼‰---
# rgb_colors, hex_colors, hsv_values, ratios = _get_colors_ex(cropped2, k=3, min_ratio=0.3)
# pill_detection.py  â€” ç„¡ rembg ç‰ˆæœ¬ï¼ˆYOLO â†’ é¡è‰²/å¤–å‹ â†’ OCRï¼‰

import os
import base64
import logging
import cv2
import torch
import numpy as np
from ultralytics import YOLO

from app.utils.image_io import read_image_safely
from openocr import OpenOCR
from app.utils.ocr_utils import recognize_with_openocr
from app.utils.shape_color_utils import (
    rotate_image_by_angle,
    enhance_contrast,
    desaturate_image,
    enhance_for_blur,
    get_basic_color_name,

    get_dominant_colors,
    increase_brightness,

    detect_shape_from_image
)

# ====== è¼•é‡åŒ–è¨­å®š ======
# Render çš„ CPU åªæœ‰ 1 æ ¸ï¼Œé¿å… PyTorch/NumPy é–‹å¤ªå¤šåŸ·è¡Œç·’
torch.set_num_threads(int(os.getenv("TORCH_NUM_THREADS", "1")))

logging.getLogger("openrec").setLevel(logging.ERROR)
ocr_engine = OpenOCR(backend='onnx', device='cpu')

DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"
_det_model = None


def get_det_model():
    """Lazy-load YOLO æ¬Šé‡ï¼Œåªåˆå§‹åŒ–ä¸€æ¬¡"""
    global _det_model
    if _det_model is None:
        print("[DET] loading YOLO modelâ€¦")
        m = YOLO("models/best.pt")

        try:
            m.fuse()
        except Exception:
            pass
        _det_model = m
        print("[DET] model ready")
    return _det_model


def generate_image_versions(base_img):
    """ç”¢ç”Ÿå¤šå€‹å½±åƒå¢å¼·ç‰ˆæœ¬ä¾› OCR å˜—è©¦"""
    # v1 = enhance_contrast(base_img, 1.5, 1.5, -0.5)
    # æ¸›å°‘åˆ¤æ–·
    # v2 = desaturate_image(v1)
    # v3 = enhance_contrast(base_img, 5.5, 2.0, -1.0)
    # v4 = desaturate_image(v3)
    # v5 = enhance_for_blur(base_img)
    # æ¸›å°‘åˆ¤æ–·
    # return [
    #     (base_img, "åŸåœ–"),
    #     (v1, "å¢å¼·1"),
    #     (v2, "å»é£½å’Œ1"),
    #     (v3, "å¢å¼·2"),
    #     (v4, "å»é£½å’Œ2"),
    #     (v5, "æ¨¡ç³Šå„ªåŒ–"),
    # ]
    # return [
    #     (base_img, "åŸåœ–"),
    #     (v1, "å¢å¼·å»é£½å’Œ"),
    # ]

    return [
        (base_img, "åŸåœ–"),

    ]


def get_best_ocr_texts(
        image_versions,
        angles=(0, 45, 90, 135, 180, 225, 270, 315), ocr_engine=None,
        # angles=(0, 90, 180, 270), ocr_engine=None,
):
    version_results = {}
    score_dict = {}

    for img_v, version_name in image_versions:
        for angle in angles:
            rotated = rotate_image_by_angle(img_v, angle)
            full_name = f"{version_name}_æ—‹è½‰{angle}"
            texts, score = recognize_with_openocr(
                rotated, ocr_engine=ocr_engine, name=full_name, min_score=0.8
            )
            version_results[full_name] = texts
            score_dict[full_name] = score

    score_combined = {
        k: (sum(len(txt) for txt in version_results[k]) * score_dict[k])
        for k in version_results
    }
    best_name = max(score_combined, key=score_combined.get)
    return version_results[best_name], best_name, score_dict[best_name]


def _fallback_rembg_crop(input_img):
    """
    Fallback crop by removing background with rembg, then take the largest blob's bbox.
    input_img: np.ndarray in BGR (as read by OpenCV)
    return: cropped np.ndarray (BGR) or None if failed
    """
    try:
        from rembg import remove
    except Exception as e:
        print(f"[REMBG] rembg not available: {e}")
        return None

    try:
        # 1) rembg returns RGBA (with alpha); keep original resolution
        rgba = remove(input_img)  # input can be np.ndarray (BGR/RGB); rembg handles internally
        if rgba is None:
            print("[REMBG] remove() returned None")
            return None

        # Ensure we have 4 channels (RGBA). If bytes returned, try decode.
        if isinstance(rgba, bytes):
            rgba = cv2.imdecode(np.frombuffer(rgba, np.uint8), cv2.IMREAD_UNCHANGED)

        if rgba is None or rgba.ndim < 3 or rgba.shape[2] < 4:
            print("[REMBG] unexpected output shape")
            return None

        # 2) alpha mask â†’ binary
        alpha = rgba[:, :, 3]
        # Heuristic binarization: Otsu + small opening/closing to clean noise
        _, mask = cv2.threshold(alpha, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

        # Morphology to remove tiny speckles and fill small holes
        kernel = np.ones((5, 5), np.uint8)
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=1)
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, iterations=2)

        # 3) find largest contour
        cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if not cnts:
            print("[REMBG] no contours found on alpha mask")
            return None

        largest = max(cnts, key=cv2.contourArea)
        x, y, w, h = cv2.boundingRect(largest)

        # sanity check: discard absurdly tiny boxes
        H, W = mask.shape[:2]
        if w * h < 0.001 * (W * H):
            print("[REMBG] contour too small; likely noise")
            return None

        # 4) crop from original BGR image (not RGBA)
        x0 = max(0, x - 5)  # small padding
        y0 = max(0, y - 5)
        x1 = min(W, x + w + 5)
        y1 = min(H, y + h + 5)

        cropped = input_img[y0:y1, x0:x1].copy()
        if cropped is None or cropped.size == 0:
            print("[REMBG] crop is empty")
            return None

        # print(f"[REMBG] crop success: ({x0},{y0})-({x1},{y1})")
        return cropped

    except Exception as e:
        print(f"[REMBG] fallback error: {e}")
        return None


def _pick_crop_from_boxes(input_img, boxes):
    """å¾ YOLO boxes é¸æœ€ä½³æ¡†ä¸¦å›å‚³è£åˆ‡åœ–ï¼ˆä¸å†å»èƒŒï¼‰"""
    xyxy = boxes.xyxy.cpu().numpy()  # [N,4]
    conf = boxes.conf.squeeze().cpu().numpy()
    conf = conf if conf.ndim else conf[None]

    areas = (xyxy[:, 2] - xyxy[:, 0]) * (xyxy[:, 3] - xyxy[:, 1])
    score = conf * (areas / (areas.max() + 1e-6))  # é¢ç©åŠ æ¬Šï¼Œé¿å…æŒ‘åˆ°è¶…å°æ¡†
    best_idx = score.argmax()
    x1, y1, x2, y2 = map(int, xyxy[best_idx])

    pad = int(0.08 * max(x2 - x1, y2 - y1))  # å°‘é‡ padding
    h, w = input_img.shape[:2]
    x1 = max(0, x1 - pad)
    y1 = max(0, y1 - pad)
    x2 = min(w - 1, x2 + pad)
    y2 = min(h - 1, y2 + pad)

    cropped = input_img[y1:y2, x1:x2]
    return cropped
import time  # ç¢ºä¿ä½ æœ‰åŠ ä¸Šé€™è¡Œ

# def process_image(img_path: str):
def process_image(image_np: np.ndarray):
    #
    print(f"[PROC] start process_image")
    t0 = time.perf_counter()
    """
    å–®å¼µè—¥å“åœ–ç‰‡è¾¨è­˜æµç¨‹ï¼š
    YOLO â†’ è£åˆ‡ â†’ é¡è‰²/å¤–å‹ â†’ å¤šç‰ˆæœ¬ OCR â†’ å›å‚³
    """

    # === Debug è¨ˆæ™‚ï¼šæ•´é«”å°åŒ…ï¼ˆç”¨ä¾†å°æ¯”å¤–å±¤ Flaskï¼‰===
    debug_start = time.perf_counter()

    # === ä¿ç•™ RGB â†’ çµ¦é¡è‰²åˆ†æç”¨ ===
    image_rgb = image_np.copy()

    # === è½‰ç‚º BGR â†’ çµ¦ YOLO/OpenCV è™•ç† ===
    input_img = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)  # âœ… é—œéµè½‰æ›
    t1 = time.perf_counter()
    print(f"â±ï¸ Pillow RGB â†’ OpenCV BGRï¼š{(t1 - t0)*1000:.1f} ms")
    print(f"â±ï¸ è®€å–åœ–ç‰‡ï¼š{(t1 - t0)*1000:.1f} ms")

    # === è®€å–æ¨¡å‹ï¼ˆå·²å¿«å–ï¼‰===
    det_model = get_det_model()
    t2 = time.perf_counter()
    print(f"â±ï¸ è®€å–æ¨¡å‹ï¼š{(t2 - t1)*1000:.1f} ms")

    # === Debug æ¨™è¨˜ï¼šé æ¸¬ä¾†æº ===
    det_src = "unknown"

    # === YOLO é æ¸¬æ™‚é–“ ===
    print("ğŸ” YOLO é–‹å§‹é æ¸¬")
    yolo_t0 = time.perf_counter()
    res = det_model.predict(
        source=input_img,
        imgsz=640,
        conf=0.25,
        iou=0.7,
        device=DEVICE,
        verbose=False
    )[0]
    yolo_t1 = time.perf_counter()
    print("âœ… YOLO çµæŸé æ¸¬")

    # === è£åˆ‡æ™‚é–“ï¼ˆå« fallbackï¼‰===
    crop_t0 = time.perf_counter()
    boxes = res.boxes
    if boxes is not None and boxes.xyxy.shape[0] > 0:
        cropped_bgr = _pick_crop_from_boxes(input_img, boxes)     # çµ¦ OCR/encode
        cropped_rgb = _pick_crop_from_boxes(image_rgb, boxes)     # çµ¦é¡è‰²åˆ†æ
        det_src = "yolo_conf_0.25"
        print("YOLO 0.25")
    else:
        res_lo = det_model.predict(
            source=input_img,
            imgsz=640,
            conf=0.10,
            iou=0.7,
            device=DEVICE,
            verbose=False
        )[0]
        boxes_lo = res_lo.boxes
        if boxes_lo is not None and boxes_lo.xyxy.shape[0] > 0:
            cropped_bgr = _pick_crop_from_boxes(input_img, boxes_lo)
            cropped_rgb = _pick_crop_from_boxes(image_rgb, boxes_lo)
            det_src = "yolo_conf_0.10"
            print("YOLO 0.10")
        else:
            cropped_bgr = _fallback_rembg_crop(input_img)
            cropped_rgb = _fallback_rembg_crop(image_rgb)
            if cropped_bgr is None or cropped_rgb is None:
                return {"error": "è—¥å“æ“·å–å¤±æ•—"}
            det_src = "rembg"
            print("REMBG")
    crop_t1 = time.perf_counter()

    t3 = crop_t1
    print(f"â±ï¸ YOLO é æ¸¬æ™‚é–“ï¼š{(yolo_t1 - yolo_t0)*1000:.1f} ms")
    print(f"â±ï¸ è£åˆ‡ï¼ˆé¸æ¡†ï¼‰æ™‚é–“ï¼š{(crop_t1 - crop_t0)*1000:.1f} ms")
    print(f"â±ï¸ YOLO åµæ¸¬+è£åˆ‡ï¼š{(t3 - t1)*1000:.1f} ms")

    # === å¤–å‹ã€é¡è‰²åˆ†æ (ç›´æ¥ç”¨è£åˆ‡åœ–, ä¸å»èƒŒ) ===
    shape, _ = detect_shape_from_image(cropped_bgr, cropped_bgr, expected_shape=None)
    t4 = time.perf_counter()
    print(f"â±ï¸ å¤–å‹åˆ†æï¼š{(t4 - t3)*1000:.1f} ms")

    # === å¤šç‰ˆæœ¬ OCR è¾¨è­˜ ===
    print("ğŸ” OCR é–‹å§‹è¾¨è­˜")
    image_versions = generate_image_versions(cropped_bgr)
    best_texts, best_name, best_score = get_best_ocr_texts(
        image_versions, ocr_engine=ocr_engine
    )
    t5 = time.perf_counter()
    print("âœ… OCR çµæŸè¾¨è­˜")
    print(f"â±ï¸ OCR å¤šç‰ˆæœ¬è¾¨è­˜ï¼š{(t5 - t4)*1000:.1f} ms")

    # === ä¸­å¤®å€åŸŸé¡è‰²åˆ†æï¼ˆæ¯”ä¾‹åˆ‡ + å…§ç¸®ï¼‰ ===
    CENTER_RATIO = 0.6   # å–çŸ­é‚Šçš„ 60% ç•¶ä¸­å¿ƒæ–¹å¡Š
    MARGIN_RATIO = 0.06  # è£åˆ‡åœ–å››é‚Šå…§ç¸® 6%

    h, w = cropped_rgb.shape[:2]
    mx, my = int(w * MARGIN_RATIO), int(h * MARGIN_RATIO)
    ix1, iy1 = mx, my
    ix2, iy2 = max(w - mx, ix1 + 1), max(h - my, iy1 + 1)
    inner = cropped_rgb[iy1:iy2, ix1:ix2].copy()

    ih, iw = inner.shape[:2]
    side = max(1, int(min(iw, ih) * CENTER_RATIO))
    cx, cy = iw // 2, ih // 2
    x1, y1 = max(cx - side // 2, 0), max(cy - side // 2, 0)
    x2, y2 = min(cx + side // 2, iw), min(cy + side // 2, ih)
    cropped2 = inner[y1:y2, x1:x2].copy()

    # === é¡è‰²åˆ†æï¼ˆä¸­å¤®å€åŸŸï¼‰===
    cropped2 = increase_brightness(cropped2, value=20)
    rgb_colors, hex_colors = get_dominant_colors(cropped2, k=3, min_ratio=0.35)
    rgb_colors_int = [tuple(map(int, c)) for c in rgb_colors]
    t6 = time.perf_counter()
    print(f"â±ï¸ ä¸­å¤®é¡è‰²åˆ†æï¼š{(t6 - t5)*1000:.1f} ms")

    # === encode æˆ base64 å‚³å›å‰ç«¯ ===
    ok, buffer = cv2.imencode(".jpg", cropped_bgr)
    cropped_b64 = (
        f"data:image/jpeg;base64,{base64.b64encode(buffer).decode('utf-8')}"
        if ok else None
    )

    # === è‰²å½©åç¨±åˆ¤å®šï¼ˆç”± RGB è½‰ HSVï¼Œå†åˆ†é¡ï¼‰===
    basic_names, hsv_values = [], []
    for rgb in rgb_colors_int:
        bgr = np.uint8([[rgb[::-1]]])
        h_raw, s, v = cv2.cvtColor(bgr, cv2.COLOR_BGR2HSV)[0][0]
        hsv_values.append((h_raw * 2, s, v))
        basic_names.append(get_basic_color_name(rgb))

    colors = list(dict.fromkeys(basic_names))
    t7 = time.perf_counter()
    print(f"â±ï¸ é¡è‰²åˆ†é¡ï¼š{(t7 - t6)*1000:.1f} ms")

    # === æœ€çµ‚çµæœè¼¸å‡º ===
    print(f"[PROC] OCR={best_texts}, shape={shape}, colors={colors}, score={best_score:.3f}")
    print(f"â±ï¸ ğŸ”š ç¸½è€—æ™‚ï¼ˆå…§éƒ¨çµ±è¨ˆï¼‰ï¼š{(t7 - t0)*1000:.1f} ms")

    debug_end = time.perf_counter()
    print(f"ğŸŸ  process_image() å¯¦éš›è€—æ™‚ï¼ˆå¤–å±¤è§€å¯Ÿï¼‰ï¼š{(debug_end - debug_start)*1000:.1f} ms")

    return {
        "æ–‡å­—è¾¨è­˜": best_texts if best_texts else ["None"],
        "æœ€ä½³ç‰ˆæœ¬": best_name,
        "ä¿¡å¿ƒåˆ†æ•¸": round(best_score, 3),
        "é¡è‰²": colors,
        "å¤–å‹": shape,
        "cropped_image": cropped_b64,
        "debug": {
            "det_source": det_src,
        }
    }
