# åˆå§‹åŒ– OpenOCR å¼•æ“
import base64

from openocr import OpenOCR
import logging

from app.utils.image_io import read_image_safely

logging.getLogger("openrec").setLevel(logging.ERROR)
ocr_engine = OpenOCR(backend='onnx', device='cpu')
# ocr_engine = OpenOCR(backend="onnx", det_model_path="models/openocr_det_model.onnx", rec_model_path="models/openocr_rec_model.onnx")


from app.utils.ocr_utils import recognize_with_openocr
from app.utils.shape_color_utils import (
    rotate_image_by_angle,
    enhance_contrast,
    desaturate_image,
    enhance_for_blur,
    extract_dominant_colors_by_ratio,
    detect_shape_from_image
)

# å¥—ç”¨å­—é«”ï¼ˆç”¨ FontPropertiesï¼‰

from matplotlib.font_manager import FontProperties

zh_font = FontProperties(fname="/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc")

from pillow_heif import register_heif_opener

register_heif_opener()



import cv2

####

def generate_image_versions(base_img):
    v1 = enhance_contrast(base_img, 1.5, 1.5, -0.5)
    v2 = desaturate_image(v1)
    v3 = enhance_contrast(base_img, 5.5, 2.0, -1.0)
    v4 = desaturate_image(v3)
    v5 = enhance_for_blur(base_img)
    return [
        (base_img, "åŸåœ–"),
        (v1, "å¢å¼·1"),
        (v2, "å»é£½å’Œ1"),
        (v3, "å¢å¼·2"),
        (v4, "å»é£½å’Œ2"),
        (v5, "æ¨¡ç³Šå„ªåŒ–")
    ]


def get_best_ocr_texts(image_versions, angles=[0, 45, 90, 135, 180, 225, 270, 315], ocr_engine=None):
    version_results = {}
    score_dict = {}
    for img_v, version_name in image_versions:
        for angle in angles:
            rotated = rotate_image_by_angle(img_v, angle)
            full_name = f"{version_name}_æ—‹è½‰{angle}"
            texts, score = recognize_with_openocr(rotated, ocr_engine=ocr_engine, name=full_name, min_score=0.8)

            # print(f"ğŸ” {full_name} => {texts} (score={score:.3f})")#è¨»è§£SSS
            version_results[full_name] = texts
            score_dict[full_name] = score

    score_combined = {
        k: sum(len(txt) for txt in version_results[k]) * score_dict[k]
        for k in version_results
    }
    best_name = max(score_combined, key=score_combined.get)
    return version_results[best_name], best_name, score_dict[best_name]


# def fallback_rembg_bounding(img_path):
# input_img = read_image_safely(img_path)
def fallback_rembg_bounding(input_img):
    if input_img is None:
        # print(f"âŒ fallback: ç„¡æ³•è®€å–åœ–ç‰‡ï¼š{img_path}")#è¨»è§£SSS
        return None, None

    try:
        rembg_img = remove(input_img)
    except Exception as e:
        print(f"âŒ rembg å»èƒŒå¤±æ•—ï¼š{e}")
        return None, None

    if rembg_img is None or rembg_img.shape[2] < 4:
        print(f"âš ï¸ rembg å›å‚³çµæœç•°å¸¸")
        return None, None

    try:
        alpha = rembg_img[:, :, 3]
        alpha = cv2.GaussianBlur(alpha, (5, 5), 0)
        _, mask = cv2.threshold(alpha, 50, 255, cv2.THRESH_BINARY)
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        if contours:
            x, y, w, h = cv2.boundingRect(max(contours, key=cv2.contourArea))
            cropped = rembg_img[y:y + h, x:x + w]
            return input_img[y:y + h, x:x + w], cropped  # è¿”å›åŸåœ–å€å¡Šã€å»èƒŒå€å¡Š
        else:
            print("âš ï¸ fallback æ²’æœ‰åµæ¸¬åˆ°è¼ªå»“")
    except Exception as e:
        print(f"â— fallback rembg bounding å‡ºéŒ¯ï¼š{e}")

    return None, None


from rembg import remove

from ultralytics import YOLO
import torch

DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"
_det_model = None


def get_det_model():
    global _det_model
    if _det_model is None:
        m = YOLO("models/best.pt")
        try:
            m.fuse()
        except Exception:
            pass
        _det_model = m
    return _det_model


def _pick_crop_from_boxes(input_img, boxes):
    """å¾ YOLO boxes é¸æœ€ä½³æ¡†ä¸¦åš paddingã€å›å‚³è£åˆ‡åœ–"""
    xyxy = boxes.xyxy.cpu().numpy()  # [N,4]
    conf = boxes.conf.squeeze().cpu().numpy()  # [N,] æˆ– scalar
    conf = conf if conf.ndim else conf[None]  # ä¿è­‰æ˜¯ä¸€ç¶­
    areas = (xyxy[:, 2] - xyxy[:, 0]) * (xyxy[:, 3] - xyxy[:, 1])
    score = conf * (areas / (areas.max() + 1e-6))  # é¢ç©åŠ æ¬Šï¼Œé¿å…æŒ‘åˆ°è¶…å°ä½†é«˜ conf çš„æ¡†
    best_idx = score.argmax()
    x1, y1, x2, y2 = map(int, xyxy[best_idx])
    # ä»¥æ¡†å¤§å°åš paddingï¼ˆ8%ï¼‰
    bw, bh = x2 - x1, y2 - y1
    pad = int(0.08 * max(bw, bh))
    h, w = input_img.shape[:2]
    x1 = max(0, x1 - pad);
    y1 = max(0, y1 - pad)
    x2 = min(w - 1, x2 + pad);
    y2 = min(h - 1, y2 + pad)
    cropped_original = input_img[y1:y2, x1:x2]
    cropped_removed = remove(cropped_original)
    return cropped_original, cropped_removed


def process_image(img_path: str):

    det_model = get_det_model()
    """
    å–®å¼µè—¥å“åœ–ç‰‡è¾¨è­˜æµç¨‹ï¼ˆæœ¬åœ° YOLOv8 + OpenOCR + é¡è‰²å¤–å‹åˆ†æï¼‰
    """
    # === è®€å–åœ–ç‰‡ ===
    input_img = read_image_safely(img_path)
    if input_img is None:
        return {"error": "ç„¡æ³•è®€å–åœ–ç‰‡"}

    # === YOLO åµæ¸¬ï¼ˆå…ˆæ­£å¸¸é–¾å€¼ï¼Œå¤±æ•—å†é™é–¾å€¼ï¼‰===
    res = det_model.predict(source=input_img, imgsz=640, conf=0.25, iou=0.7,
                            device=DEVICE, verbose=False)[0]
    boxes = res.boxes

    if boxes is not None and boxes.xyxy.shape[0] > 0:
        cropped_original, cropped_removed = _pick_crop_from_boxes(input_img, boxes)
    else:
        res_lo = det_model.predict(source=input_img, imgsz=640, conf=0.10, iou=0.7,
                                   device=DEVICE, verbose=False)[0]
        boxes_lo = res_lo.boxes
        if boxes_lo is not None and boxes_lo.xyxy.shape[0] > 0:
            cropped_original, cropped_removed = _pick_crop_from_boxes(input_img, boxes_lo)
        else:
            # æœ€å¾Œæ‰èµ° rembg fallback
            cropped_original, cropped_removed = fallback_rembg_bounding(input_img)
            if cropped_removed is None:
                return {"error": "è—¥å“æ“·å–å¤±æ•—"}

    # === è£åˆ‡åœ–è½‰ Base64ï¼ˆçµ¦å‰ç«¯é¡¯ç¤ºï¼‰ ===
    ok, buffer = cv2.imencode(".jpg", cropped_original)
    cropped_b64 = f"data:image/jpeg;base64,{base64.b64encode(buffer).decode('utf-8')}" if ok else None

    # === å¤–å‹ã€é¡è‰²åˆ†æ ===
    shape, _ = detect_shape_from_image(cropped_removed, cropped_original, expected_shape=None, debug=False)
    colors = extract_dominant_colors_by_ratio(cropped_removed, visualize=False)

    # === å¤šç‰ˆæœ¬ OCR è¾¨è­˜ ===
    image_versions = generate_image_versions(cropped_removed)
    best_texts, best_name, best_score = get_best_ocr_texts(image_versions, ocr_engine=ocr_engine)

    print("æ–‡å­—è¾¨è­˜ï¼š", best_texts if best_texts else ["None"])
    print("æœ€ä½³ç‰ˆæœ¬ï¼š", best_name)
    print("ä¿¡å¿ƒåˆ†æ•¸ï¼š", round(best_score, 3))
    print("é¡è‰²ï¼š", colors)
    print("å¤–å‹ï¼š", shape)

    return {
        "æ–‡å­—è¾¨è­˜": best_texts if best_texts else ["None"],
        "æœ€ä½³ç‰ˆæœ¬": best_name,
        "ä¿¡å¿ƒåˆ†æ•¸": round(best_score, 3),
        "é¡è‰²": colors,
        "å¤–å‹": shape,

        "cropped_image": cropped_b64
    }
