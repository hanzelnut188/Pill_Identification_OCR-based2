# åˆå§‹åŒ– OpenOCR å¼•æ“
from openocr import OpenOCR
import logging

from app.utils.image_io import read_image_safely

logging.getLogger("openrec").setLevel(logging.ERROR)
ocr_engine = OpenOCR(backend='onnx', device='cpu')
# ocr_engine = OpenOCR(backend="onnx", det_model_path="models/openocr_det_model.onnx", rec_model_path="models/openocr_rec_model.onnx")


from app.utils.ocr_utils import recognize_with_openocr
from app.utils.shape_color_utils import (
    rotate_image_by_angle,
    enhance_contrast,
    desaturate_image,
    enhance_for_blur,
    extract_dominant_colors_by_ratio,
    detect_shape_from_image
)

# å¥—ç”¨å­—é«”ï¼ˆç”¨ FontPropertiesï¼‰

from matplotlib.font_manager import FontProperties


zh_font = FontProperties(fname="/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc")


from pillow_heif import register_heif_opener

register_heif_opener()

from inference_sdk import InferenceHTTPClient

import cv2

from rembg import remove
from ultralytics import YOLO
det_model = YOLO("/path/to/best.pt")

CLIENT = InferenceHTTPClient(
    api_url="https://detect.roboflow.com",
    api_key="SOlzinVqG2xuWsPUUGRp"
    # api_key="kylIYUWNLWHPy2RXUVOe"
)
MODEL_ID = "pill-detection-poc-i0b3g/1"


####

def generate_image_versions(base_img):
    v1 = enhance_contrast(base_img, 1.5, 1.5, -0.5)
    v2 = desaturate_image(v1)
    v3 = enhance_contrast(base_img, 5.5, 2.0, -1.0)
    v4 = desaturate_image(v3)
    v5 = enhance_for_blur(base_img)
    return [
        (base_img, "åŸåœ–"),
        (v1, "å¢å¼·1"),
        (v2, "å»é£½å’Œ1"),
        (v3, "å¢å¼·2"),
        (v4, "å»é£½å’Œ2"),
        (v5, "æ¨¡ç³Šå„ªåŒ–")
    ]


def get_best_ocr_texts(image_versions, angles=[0, 45, 90, 135, 180, 225, 270, 315], ocr_engine=None):
    version_results = {}
    score_dict = {}
    for img_v, version_name in image_versions:
        for angle in angles:
            rotated = rotate_image_by_angle(img_v, angle)
            full_name = f"{version_name}_æ—‹è½‰{angle}"
            texts, score = recognize_with_openocr(rotated, ocr_engine=ocr_engine, name=full_name, min_score=0.8)

            # print(f"ğŸ” {full_name} => {texts} (score={score:.3f})")#è¨»è§£SSS
            version_results[full_name] = texts
            score_dict[full_name] = score

    score_combined = {
        k: sum(len(txt) for txt in version_results[k]) * score_dict[k]
        for k in version_results
    }
    best_name = max(score_combined, key=score_combined.get)
    return version_results[best_name], best_name, score_dict[best_name]


def get_bbox_from_rembg_alpha(img_path):
    input_img = cv2.imread(img_path)
    rembg_img = remove(input_img)

    if rembg_img.shape[2] == 4:
        alpha = rembg_img[:, :, 3]
        alpha = cv2.GaussianBlur(alpha, (5, 5), 0)
        _, mask = cv2.threshold(alpha, 50, 255, cv2.THRESH_BINARY)
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        if contours:
            x, y, w, h = cv2.boundingRect(max(contours, key=cv2.contourArea))
            return rembg_img, (x, y, w, h)  # âœ return cropped image & bounding box
    return None, None


# === æ¨¡çµ„åŒ–ï¼šå¾å®Œæ•´åœ–ç‰‡èˆ‡åµæ¸¬æ¡†ä¸­æ“·å–è—¥ç‰©å€åŸŸ ===
# def extract_pill_region(img_path, detection_result, margin=10):
def extract_pill_region(input_img, detection_result, margin=10):
    #    input_img = read_image_safely(img_path)
    if input_img is None:
        # print(f"âŒ extract_pill_region: ç„¡æ³•è®€å–åœ–ç‰‡ï¼š{img_path}")#è¨»è§£SSS
        return None, None

    try:
        h_img, w_img = input_img.shape[:2]
        cx, cy = detection_result["x"], detection_result["y"]
        bw, bh = detection_result["width"], detection_result["height"]

        x0 = max(0, int(cx - bw / 2) - margin)
        y0 = max(0, int(cy - bh / 2) - margin)
        x1 = min(w_img, int(cx + bw / 2) + margin)
        y1 = min(h_img, int(cy + bh / 2) + margin)

        cropped_original = input_img[y0:y1, x0:x1]

        try:
            cropped_removed = remove(cropped_original)
        except Exception as e:
            # print(f"âŒ rembg å»èƒŒå¤±æ•—ï¼š{e}")#è¨»è§£SSS
            return cropped_original, None

        return cropped_original, cropped_removed

    except Exception as e:
        # print(f"â— extract_pill_region éŒ¯èª¤ï¼š{e}")#è¨»è§£SSS
        return None, None


# def fallback_rembg_bounding(img_path):
# input_img = read_image_safely(img_path)
def fallback_rembg_bounding(input_img):
    if input_img is None:
        # print(f"âŒ fallback: ç„¡æ³•è®€å–åœ–ç‰‡ï¼š{img_path}")#è¨»è§£SSS
        return None, None

    try:
        rembg_img = remove(input_img)
    except Exception as e:
        print(f"âŒ rembg å»èƒŒå¤±æ•—ï¼š{e}")
        return None, None

    if rembg_img is None or rembg_img.shape[2] < 4:
        print(f"âš ï¸ rembg å›å‚³çµæœç•°å¸¸")
        return None, None

    try:
        alpha = rembg_img[:, :, 3]
        alpha = cv2.GaussianBlur(alpha, (5, 5), 0)
        _, mask = cv2.threshold(alpha, 50, 255, cv2.THRESH_BINARY)
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        if contours:
            x, y, w, h = cv2.boundingRect(max(contours, key=cv2.contourArea))
            cropped = rembg_img[y:y + h, x:x + w]
            return input_img[y:y + h, x:x + w], cropped  # è¿”å›åŸåœ–å€å¡Šã€å»èƒŒå€å¡Š
        else:
            print("âš ï¸ fallback æ²’æœ‰åµæ¸¬åˆ°è¼ªå»“")
    except Exception as e:
        print(f"â— fallback rembg bounding å‡ºéŒ¯ï¼š{e}")

    return None, None




from ultralytics import YOLO

# âœ… å…¨åŸŸè¼‰å…¥æœ¬åœ° YOLO æ¨¡å‹ï¼ˆè«‹æå‰åˆå§‹åŒ–ä¸€æ¬¡ï¼‰
det_model = YOLO("/path/to/best.pt")  # æ›¿æ›æˆä½ çš„ Roboflow åŒ¯å‡º .pt è·¯å¾‘


def process_image(img_path: str):
    """
    å–®å¼µè—¥å“åœ–ç‰‡è¾¨è­˜æµç¨‹ï¼ˆæœ¬åœ° YOLOv8 + OpenOCR + é¡è‰²å¤–å‹åˆ†æï¼‰
    """
    from PIL import Image
    import base64

    # === è®€å–åœ–ç‰‡ ===
    input_img = read_image_safely(img_path)
    if input_img is None:
        return {"error": "ç„¡æ³•è®€å–åœ–ç‰‡"}

    # === ä½¿ç”¨æœ¬åœ° YOLO æ¨¡å‹é€²è¡Œæ¨è«– ===
    results = det_model(input_img)

    # === è™•ç† YOLO åµæ¸¬çµæœ ===
    preds = results[0].boxes
    if preds and len(preds) > 0:
        # å–æœ€å¤§æ¡†ï¼ˆä¿¡å¿ƒåˆ†æ•¸æœ€é«˜çš„ï¼‰
        boxes = preds.xyxy.cpu().numpy()  # [x1, y1, x2, y2]
        best_idx = preds.conf.argmax().item()
        box = boxes[best_idx]
        x1, y1, x2, y2 = map(int, box)
        cropped_original = input_img[y1:y2, x1:x2]
        cropped_removed = remove(cropped_original)  # ä»ä½¿ç”¨ rembg å»èƒŒ
    else:
        # YOLO åµæ¸¬ä¸åˆ° âœ fallback
        cropped_original, cropped_removed = fallback_rembg_bounding(input_img)
        if cropped_removed is None:
            return {"error": "è—¥å“æ“·å–å¤±æ•—"}

    # === è£åˆ‡åœ–è½‰ Base64 çµ¦å‰ç«¯å±•ç¤º ===
    _, buffer = cv2.imencode(".jpg", cropped_original)
    cropped_base64 = base64.b64encode(buffer).decode("utf-8")
    cropped_base64 = f"data:image/jpeg;base64,{cropped_base64}"

    # === å¤–å‹ã€é¡è‰²åˆ†æ ===
    shape, _ = detect_shape_from_image(cropped_removed, cropped_original, expected_shape=None, debug=False)
    colors = extract_dominant_colors_by_ratio(cropped_removed, visualize=False)

    # === å¤šç‰ˆæœ¬ OCR è¾¨è­˜ ===
    image_versions = generate_image_versions(cropped_removed)
    best_texts, best_name, best_score = get_best_ocr_texts(image_versions, ocr_engine=ocr_engine)

    print("æ–‡å­—è¾¨è­˜ï¼š" + str(best_texts if best_texts else ["None"]))
    print("æœ€ä½³ç‰ˆæœ¬ï¼š" + str(best_name))
    print("ä¿¡å¿ƒåˆ†æ•¸ï¼š" + str(round(best_score, 3)))
    print("é¡è‰²ï¼š" + str(colors))
    print("å¤–å‹ï¼š" + str(shape))

    return {
        "æ–‡å­—è¾¨è­˜": best_texts if best_texts else ["None"],
        "æœ€ä½³ç‰ˆæœ¬": best_name,
        "ä¿¡å¿ƒåˆ†æ•¸": round(best_score, 3),
        "é¡è‰²": colors,
        "å¤–å‹": shape,
        "cropped_image": cropped_base64
    }

