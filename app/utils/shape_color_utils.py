import cv2


def rotate_image_by_angle(image, angle):
    """
    å°‡åœ–ç‰‡ä¾æŒ‡å®šè§’åº¦æ—‹è½‰ã€‚
    - image: è¼¸å…¥åœ–ç‰‡ï¼ˆOpenCV æ ¼å¼ï¼‰
    - angle: é †æ™‚é‡æ—‹è½‰è§’åº¦ï¼ˆä¾‹å¦‚ 90, 180ï¼‰
    - return: æ—‹è½‰å¾Œçš„åœ–ç‰‡
    """
    (h, w) = image.shape[:2]
    center = (w // 2, h // 2)
    M = cv2.getRotationMatrix2D(center, angle, 1.0)
    rotated = cv2.warpAffine(image, M, (w, h), borderMode=cv2.BORDER_REPLICATE)
    return rotated


def enhance_contrast(img, clip_limit, alpha, beta):
    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=(8, 8))
    cl = clahe.apply(l)
    merged = cv2.merge((cl, a, b))
    enhance_img = cv2.cvtColor(merged, cv2.COLOR_LAB2BGR)
    blurred = cv2.GaussianBlur(enhance_img, (5, 5), 3.0)
    return cv2.addWeighted(enhance_img, alpha, blurred, beta, 0)


# def extract_dominant_colors_by_ratio(cropped_img, k=4, min_ratio=0.38):
    # import colorsys
    # import numpy as np
    # import cv2
    # from collections import Counter

    # def rgb_to_color_group(rgb):
    #     r, g, b = rgb / 255.0
    #     h, s, v = colorsys.rgb_to_hsv(r, g, b)
    #     h_deg = h * 360
    #     if v < 0.2:
    #         return "é»‘è‰²"
    #     if s < 0.1 and v > 0.9:
    #         return "ç™½è‰²"
    #     if s < 0.05 and v > 0.6:
    #         return "é€æ˜"
    #     if h_deg < 15 or h_deg >= 345:
    #         return "ç´…è‰²"
    #     elif h_deg < 40:
    #         return "æ©˜è‰²"
    #     elif h_deg < 55:
    #         return "çš®è†šè‰²"
    #     elif h_deg < 65:
    #         return "é»ƒè‰²"
    #     elif h_deg < 170:
    #         return "ç¶ è‰²"
    #     elif h_deg < 250:
    #         return "è—è‰²"
    #     elif h_deg < 290:
    #         return "ç´«è‰²"
    #     elif h_deg < 345:
    #         return "ç²‰ç´…è‰²"
    #     if s > 0.2 and v < 0.5:
    #         return "æ£•è‰²"
    #     return "æœªçŸ¥"

    # similar_color_map = {
    #     "çš®è†šè‰²": "é»ƒè‰²",
    #     "æ©˜è‰²": "ç´…è‰²",
    #     "ç²‰ç´…è‰²": "ç´…è‰²",
    #     "é€æ˜": "ç™½è‰²",
    #     "æ£•è‰²": "é»‘è‰²",
    # }

    # # â†“ å°åœ–ï¼‹å–æ¨£ï¼Œæ¸›å°‘è¨ˆç®—é‡
    # img_rgb = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB)
    # resized = cv2.resize(img_rgb, (48, 48), interpolation=cv2.INTER_AREA)
    # pixels = resized.reshape(-1, 3)
    # # å»æ‰éå¸¸æš—çš„åƒç´ ï¼ˆèƒŒæ™¯/é™°å½±ï¼‰
    # pixels = pixels[np.sum(pixels, axis=1) > 30]

    # # å†æ¬¡éš¨æ©Ÿå–æ¨£æœ€å¤š 1500 å€‹é»ï¼Œè¶³å¤ ç©©å®š
    # if len(pixels) > 1500:
    #     idx = np.random.choice(len(pixels), 1500, replace=False)
    #     pixels = pixels[idx]

    # # OpenCV KMeansï¼ˆfloat32ï¼‰
    # Z = np.float32(pixels)
    # criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)
    # attempts = 1
    # compactness, labels, centers = cv2.kmeans(
    #     Z, k, None, criteria, attempts, cv2.KMEANS_PP_CENTERS
    # )
    # labels = labels.flatten()
    # centers = centers.astype(np.float32)

    # # çµ±è¨ˆæ¯ç¾¤å æ¯”
    # counts = np.bincount(labels, minlength=k).astype(np.float32)
    # total = counts.sum() if counts.sum() > 0 else 1.0

    # # å°æ¯ç¾¤åšèªæ„è‰²æ˜ å°„
    # semantic_counter = Counter()
    # for i, cnt in enumerate(counts):
    #     color = rgb_to_color_group(centers[i])
    #     if color not in ("æœªçŸ¥", "é€æ˜"):
    #         semantic_counter[color] += cnt

    # # å–ä¸»è‰²ï¼ˆæœ€å¤š 2 ç¨®ã€å æ¯” >= min_ratioï¼‰
    # items = [(c, v / total) for c, v in semantic_counter.items()]
    # items.sort(key=lambda x: -x[1])
    # dominant = [c for c, r in items if r >= min_ratio][:2]

    # # æ“´å……ç›¸è¿‘è‰²ï¼ˆä¸é‡è¤‡ï¼‰
    # extended = dominant.copy()
    # for c in dominant:
    #     sim = similar_color_map.get(c)
    #     if sim and sim not in extended:
    #         extended.append(sim)

    # return extended


def get_dominant_colors(image, k=3, ignore_black=True, min_ratio=0.3):
    """Extract dominant colors using KMeans, ignore black and small/shadow clusters."""
    # resize for speed
    img = cv2.resize(image, (600, 400))
    img_flat = img.reshape((-1, 3))

    # run kmeans
    kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)
    labels = kmeans.fit_predict(img_flat)

    counts = Counter(labels)
    centers = kmeans.cluster_centers_

    # order by frequency
    ordered = counts.most_common()
    ordered_colors = [centers[i] for i, _ in ordered]

    # filter out black if requested
    if ignore_black:
        def is_black(c, threshold=40):  # v < 40 means black
            bgr = np.uint8([[c[::-1]]])
            h, s, v = cv2.cvtColor(bgr, cv2.COLOR_BGR2HSV)[0][0]
            return v < threshold
        ordered_colors = [c for c in ordered_colors if not is_black(c)]

    # ---- merge similar colors ----
    merged_colors = []
    for idx, c in enumerate(ordered_colors):
        bgr = np.uint8([[c[::-1]]])
        h_raw, s, v = cv2.cvtColor(bgr, cv2.COLOR_BGR2HSV)[0][0]
        h_deg = h_raw * 2
        hsv_c = (h_deg, int(s), int(v))

        merged = False
        for mc in merged_colors:
            if is_color_similar(hsv_c, mc["hsv"]):
                mc["count"] += counts[ordered[idx][0]]
                merged = True
                break

        if not merged:
            merged_colors.append({
                "rgb": tuple(map(int, c)),
                "hsv": hsv_c,
                "count": counts[ordered[idx][0]]
            })

    # ---- filter out small clusters (shadows, noise) ----
    total = sum(mc["count"] for mc in merged_colors)
    filtered_colors = [mc for mc in merged_colors if mc["count"] / total >= min_ratio]

    # safeguard: if all removed, keep the largest one
    if not filtered_colors and merged_colors:
        filtered_colors = [max(merged_colors, key=lambda mc: mc["count"])]

    hex_colors = [rgb_to_hex(mc["rgb"]) for mc in filtered_colors]

    return [mc["rgb"] for mc in filtered_colors], hex_colors
    

def get_basic_color_name(rgb):
    """Classify an RGB value into a basic color family (Chinese Traditional)."""
    
    # Convert RGB (0â€“255) to HSV
    bgr = np.uint8([[rgb[::-1]]])  # OpenCV expects BGR
    hsv = cv2.cvtColor(bgr, cv2.COLOR_BGR2HSV)[0][0]
    h, s, v = hsv
    h = int(h) * 2  # Convert to degrees (0-360)
    r, g, b = rgb

    # Handle black/white/gray
    # if v < 60:   # instead of 40 â†’ more tolerant to lighting
    #     return "é»‘è‰²"
    if s < 40 and v > 170:
        return "ç™½è‰²"
    if s < 40:
        return "ç°è‰²"

    # Hue ranges
    if (h < 10 or h >= 330) and s > 90 and r > 50:
        return "ç´…è‰²"
    elif h < 30:
        return "æ£•è‰²" if v < 150 else "æ©™è‰²"
    elif h < 60:
        return "é»ƒè‰²"
    elif h < 250 and g > b:
        return "ç¶ è‰²"
    elif h < 250 and g < b:
        return "è—è‰²"
    elif h < 300:  # candidate for pink (270â€“330)
        return "ç´«è‰²"
    elif h < 360:  # candidate for pink (270â€“330)
        return "ç²‰ç´…è‰²"            
    else:
        return "å…¶ä»–"
        
# === å¢å¼·è™•ç†å‡½å¼ ===

def desaturate_image(img):
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    h, s, v = cv2.split(hsv)
    s.fill(0)
    return cv2.cvtColor(cv2.merge([h, s, v]), cv2.COLOR_HSV2BGR)


def enhance_for_blur(img):
    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=3.5, tileGridSize=(8, 8))
    cl = clahe.apply(l)
    enhanced_lab = cv2.merge((cl, a, b))
    contrast_img = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)
    blurred = cv2.GaussianBlur(contrast_img, (3, 3), 1.0)
    sharpened = cv2.addWeighted(contrast_img, 1.8, blurred, -0.8, 0)
    return cv2.bilateralFilter(sharpened, d=9, sigmaColor=75, sigmaSpace=75)


# === 3. å®šç¾©è®€å– HEIC çš„å‡½å¼ ===
# === å¯èª¿åƒæ•¸ï¼ˆé è¨­å€¼æ”¾ä½ ç›®å‰æœ€ä½³ï¼‰===
CIRCLE_LO = 1
CIRCLE_HI = 1.2
ELLIPSE_HI = 3.8


def set_shape_thresholds(circle_lo: float, circle_hi: float, ellipse_hi: float):
    global CIRCLE_LO, CIRCLE_HI, ELLIPSE_HI
    CIRCLE_LO = circle_lo
    CIRCLE_HI = circle_hi
    ELLIPSE_HI = ellipse_hi


def detect_shape_three_classes(contour, expected_shape=None):
    shape = "å…¶ä»–"
    try:
        if len(contour) >= 5:
            ellipse = cv2.fitEllipse(contour)
            (center, axes, angle) = ellipse
            major, minor = axes
            if minor == 0:
                return shape

            ratio = max(major, minor) / min(major, minor)
            ratios_list.append(ratio)

            # === classify with global thresholds ===
            if CIRCLE_LO <= ratio <= CIRCLE_HI:
                shape = "åœ“å½¢"
            elif ratio <= ELLIPSE_HI:
                shape = "æ©¢åœ“å½¢"
            else:
                shape = "å…¶ä»–"


    except Exception as e:
        print(f"â— detect_shape_three_classes ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
    return shape


# def preprocess_with_shadow_correction(img_bgr):
#     """æ ¡æ­£é™°å½±èˆ‡è‡ªå‹•äºŒå€¼åŒ–ï¼Œæ”¹å–„è¼ªå»“å“è³ª"""
#     # Step 1: ç°éšè½‰æ›
#     gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
#
#     # Step 2: é«˜æ–¯æ¨¡ç³Šè¨ˆç®—èƒŒæ™¯äº®åº¦
#     blur = cv2.GaussianBlur(gray, (55, 55), 0)
#
#     # Step 3: ç°éšé™¤ä»¥èƒŒæ™¯äº®åº¦ => ä¿®æ­£é™°å½±
#     corrected = cv2.divide(gray, blur, scale=255)
#
#     # Step 4: è‡ªé©æ‡‰ threshold => é©åˆå±€éƒ¨äº®åº¦è®ŠåŒ–
#     thresh = cv2.adaptiveThreshold(
#         corrected, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
#         cv2.THRESH_BINARY, 21, 10
#     )
#
#     return thresh
def preprocess_with_shadow_correction(img_bgr):
    """æ”¹é€²çš„å‰è™•ç†ï¼Œæ›´å¥½åœ°åˆ†é›¢è—¥ç‰©èˆ‡èƒŒæ™¯"""
    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)

    # å¤šå°ºåº¦çš„èƒŒæ™¯ä¼°è¨ˆ
    blur1 = cv2.GaussianBlur(gray, (25, 25), 0)
    blur2 = cv2.GaussianBlur(gray, (75, 75), 0)

    # é›™é‡é™°å½±æ ¡æ­£
    corrected1 = cv2.divide(gray, blur1, scale=255)
    corrected2 = cv2.divide(gray, blur2, scale=255)
    corrected = cv2.addWeighted(corrected1, 0.5, corrected2, 0.5, 0)

    # ä½¿ç”¨ OTSU è‡ªå‹•æ‰¾æœ€ä½³é–¾å€¼
    _, otsu_thresh = cv2.threshold(corrected, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

    # å½¢æ…‹å­¸æ“ä½œå»é™¤é›œè¨Š
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
    cleaned = cv2.morphologyEx(otsu_thresh, cv2.MORPH_CLOSE, kernel)
    cleaned = cv2.morphologyEx(cleaned, cv2.MORPH_OPEN, kernel)

    return cleaned


def detect_shape_from_image(cropped_img, original_img=None, expected_shape=None):
    try:
        output = cropped_img.copy()
        thresh = preprocess_with_shadow_correction(output)
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
        shape = "å…¶ä»–"
        if not contours and original_img is not None:
            gray_fallback = cv2.cvtColor(original_img, cv2.COLOR_BGR2GRAY)
            _, thresh_fallback = cv2.threshold(gray_fallback, 127, 255, cv2.THRESH_BINARY)
            contours_fallback, _ = cv2.findContours(thresh_fallback, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if contours_fallback:
                main_contour = max(contours_fallback, key=cv2.contourArea)
                shape = detect_shape_three_classes(main_contour, expected_shape=expected_shape)
        elif contours:
            main_contour = max(contours, key=cv2.contourArea)
            shape = detect_shape_three_classes(main_contour, expected_shape=expected_shape)

        if expected_shape:
            result = "âœ…" if shape == expected_shape else "âŒ"
            return shape, result
        return shape, None
    except Exception as e:
        print(f"â— ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return "éŒ¯èª¤", None


ratios_list = []

#########################################


###æ¸¬è©¦å¾Œ
# def detect_shape_from_image(cropped_img, original_img=None, expected_shape=None):
#     try:
#         output = cropped_img.copy()
#         thresh = preprocess_with_shadow_correction(output)
#
#         contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
#         shape = "å…¶ä»–"
#
#         if not contours and original_img is not None:
#             # print("âš ï¸ ç„¡åµæ¸¬åˆ°è¼ªå»“ï¼Œæ”¹ç”¨åŸåœ–å˜—è©¦")#è¨»è§£SSS
#             gray_fallback = cv2.cvtColor(original_img, cv2.COLOR_BGR2GRAY)
#             _, thresh_fallback = cv2.threshold(gray_fallback, 127, 255, cv2.THRESH_BINARY)
#             contours_fallback, _ = cv2.findContours(thresh_fallback, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
#
#             if contours_fallback:
#                 main_contour = max(contours_fallback, key=cv2.contourArea)
#                 shape = detect_shape_three_classes(main_contour)
#             else:
#                 print("âš ï¸ äºŒæ¬¡å˜—è©¦ä»ç„¡è¼ªå»“ï¼Œæ¨™è¨˜ç‚ºå…¶ä»–")  # è¨»è§£SSS
#         elif contours:
#             main_contour = max(contours, key=cv2.contourArea)
#             area = cv2.contourArea(main_contour)
#             img_area = cropped_img.shape[0] * cropped_img.shape[1]
#             area_ratio = area / img_area
#             # print(f"ğŸ“ è¼ªå»“é¢ç©ï¼š{area:.1f}ï¼Œåœ–ç‰‡é¢ç©ï¼š{img_area:.1f}ï¼Œä½”æ¯”ï¼š{area_ratio:.2%}")#è¨»è§£SSS
#             shape = detect_shape_three_classes(main_contour)
#
#         if expected_shape:
#             result = "âœ…" if shape == expected_shape else "âŒ"
#             # print(f"ğŸ“ é æ¸¬çµæœï¼š{shape}ï¼Œæ­£ç¢ºçµæœï¼š{expected_shape} {result}")#è¨»è§£SSS
#             return shape, result
#         return shape, None
#
#     except Exception as e:
#         print(f"â— ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")  # è¨»è§£SSS
#         return "éŒ¯èª¤", None
# === æ”¾åœ¨æª”æ¡ˆé ‚éƒ¨å®šç¾©çµ±è¨ˆç”¨æ¸…å–® ===
# ratios_list = []
#
#
# æ¸¬è©¦å‰
# def detect_shape_three_classes(contour):
#     shape = "å…¶ä»–"
#     # print(len(contour))#è¨»è§£SSS
#     try:
#         if len(contour) >= 5:
#             ellipse = cv2.fitEllipse(contour)
#             (center, axes, angle) = ellipse
#             major, minor = axes
#
#             if minor == 0:
#                 return shape
#
#             ratio = max(major, minor) / min(major, minor)
#             ratios_list.append(ratio)
#             # print(f"ğŸ” Ellipse ratio: {ratio:.3f}")#è¨»è§£SSS
#
#             # â¤ åˆ†é¡
#             if 0.95 <= ratio <= 1.15:
#                 shape = "åœ“å½¢"
#             elif ratio <= 2.3:
#                 shape = "æ©¢åœ“å½¢"
#             else:
#                 shape = "å…¶ä»–"
#
#             # print(f"ğŸ“ shape ratio: {ratio:.2f} => åˆ¤æ–·ç‚º {shape}")
#
#     except  Exception as e:
#         print(f"â— detect_shape_three_classes ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
#
#     return shape
